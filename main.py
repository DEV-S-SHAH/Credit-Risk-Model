import sys
import subprocess
import importlib
import datetime
from datetime import timedelta
import random
import warnings
import os
import time
from collections import defaultdict

# Suppress warnings early
warnings.filterwarnings('ignore')

def install_package(package):
    """Install a package using pip."""
    print(f"Installing {package}...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

# Check and install required packages
required_packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'streamlit', 
                    'shap', 'plotly', 'scikit-learn', 'xgboost', 'joblib', 
                    'lightgbm', 'catboost', 'imbalanced-learn']

for package in required_packages:
    try:
        importlib.import_module(package.replace('-', '_'))
    except ImportError:
        install_package(package)

# Now import all required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import shap
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score, 
                           confusion_matrix, classification_report, accuracy_score,
                           precision_recall_curve, roc_curve)
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
from imblearn.ensemble import BalancedRandomForestClassifier
import joblib

# Set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

# Set style for plots
plt.style.use('fivethirtyeight')
sns.set_palette('Set2')

class CreditRiskModel:
    """Main class for the Credit Risk Modelling project"""
    
    def __init__(self):
        self.data = None
        self.model = None
        self.scaler = None
        self.label_encoders = {}
        self.feature_names = None
        self.model_comparison = None
        self.best_model_name = None
        self.feature_importance_df = None
        
    def create_comprehensive_dataset(self, n_users=5000):
        """Create a comprehensive realistic dataset"""
        print("Creating comprehensive credit risk dataset...")
        
        np.random.seed(42)
        
        # Basic Demographics
        user_ids = list(range(1, n_users + 1))
        ages = np.random.normal(35, 12, n_users).astype(int)
        ages = np.clip(ages, 21, 65)
        
        # Income with realistic distribution
        incomes = np.random.lognormal(11, 0.6, n_users).astype(int)
        incomes = np.clip(incomes, 15000, 300000)
        
        cities = np.random.choice(
            ['Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata', 'Hyderabad', 'Pune', 'Ahmedabad'], 
            n_users, 
            p=[0.18, 0.16, 0.15, 0.12, 0.10, 0.12, 0.09, 0.08]
        )
        
        occupations = np.random.choice(
            ['Salaried', 'Self-employed', 'Business', 'Freelancer', 'Professional'], 
            n_users, 
            p=[0.50, 0.20, 0.15, 0.10, 0.05]
        )
        
        # Education level
        education = np.random.choice(
            ['Graduate', 'Post-Graduate', 'Undergraduate', 'Doctorate'],
            n_users,
            p=[0.45, 0.30, 0.20, 0.05]
        )
        
        # Employment length in years
        employment_length = np.random.gamma(3, 2, n_users).astype(int)
        employment_length = np.clip(employment_length, 0, 40)
        
        # Existing loans
        existing_loans = np.random.choice([0, 1, 2, 3, 4, 5], n_users, p=[0.25, 0.35, 0.20, 0.12, 0.06, 0.02])
        
        # Credit history length (months)
        credit_history_length = np.random.gamma(5, 12, n_users).astype(int)
        credit_history_length = np.clip(credit_history_length, 6, 240)
        
        # === UPI Transaction Features ===
        # Monthly UPI transaction count
        upi_transaction_count = np.random.gamma(4, 8, n_users).astype(int)
        upi_transaction_count = np.clip(upi_transaction_count, 5, 200)
        
        # Average UPI transaction amount
        avg_upi_amount = np.random.lognormal(6, 0.7, n_users).astype(int)
        avg_upi_amount = np.clip(avg_upi_amount, 100, 15000)
        
        # Monthly UPI spend
        monthly_upi_spend = upi_transaction_count * avg_upi_amount * np.random.uniform(0.7, 1.3, n_users)
        monthly_upi_spend = monthly_upi_spend.astype(int)
        monthly_upi_spend = np.clip(monthly_upi_spend, 500, 100000)
        
        # Transaction volatility (coefficient of variation)
        transaction_volatility = np.random.beta(2, 5, n_users)
        
        # Income ratio (incoming vs outgoing)
        income_ratio = np.random.beta(4, 6, n_users)
        
        # UPI merchant diversity (1-10)
        merchant_diversity = np.random.poisson(5, n_users)
        merchant_diversity = np.clip(merchant_diversity, 1, 10)
        
        # === E-commerce Features ===
        # Monthly e-commerce spend
        monthly_ecom_spend = np.random.lognormal(8.5, 0.9, n_users).astype(int)
        monthly_ecom_spend = np.clip(monthly_ecom_spend, 1000, 80000)
        
        # Average cart value
        avg_cart_value = np.random.lognormal(7, 0.6, n_users).astype(int)
        avg_cart_value = np.clip(avg_cart_value, 500, 25000)
        
        # Number of monthly orders
        num_orders = (monthly_ecom_spend / avg_cart_value * np.random.uniform(0.8, 1.2, n_users)).astype(int)
        num_orders = np.clip(num_orders, 1, 40)
        
        # Return rate
        return_rate = np.random.beta(2, 15, n_users)
        
        # Category diversity (1-8)
        category_diversity = np.random.poisson(4, n_users)
        category_diversity = np.clip(category_diversity, 1, 8)
        
        # Payment preference
        payment_preference = np.random.choice(
            ['UPI', 'Credit Card', 'Debit Card', 'COD', 'EMI'],
            n_users,
            p=[0.40, 0.25, 0.20, 0.10, 0.05]
        )
        
        # === Investment Features ===
        # Number of stocks/mutual funds
        num_stocks = np.random.poisson(6, n_users)
        num_stocks = np.clip(num_stocks, 0, 50)
        
        # Total investment
        total_investment = (incomes * np.random.uniform(0.5, 5, n_users)).astype(int)
        total_investment = np.clip(total_investment, 0, 2000000)
        
        # Portfolio risk (0-1, where 0 is low risk, 1 is high risk)
        portfolio_risk = np.random.beta(3, 4, n_users)
        
        # Average holding period in days
        avg_holding_period = np.random.gamma(3, 40, n_users).astype(int)
        avg_holding_period = np.clip(avg_holding_period, 1, 730)
        
        # Profit/Loss ratio
        profit_loss_ratio = np.random.normal(0.12, 0.25, n_users)
        profit_loss_ratio = np.clip(profit_loss_ratio, -0.6, 0.8)
        
        # SIP (Systematic Investment Plan) active
        sip_active = np.random.choice([0, 1], n_users, p=[0.6, 0.4])
        
        # === Behavioral Features ===
        # Late payment count in last 12 months
        late_payment_count = np.random.poisson(1.5, n_users)
        late_payment_count = np.clip(late_payment_count, 0, 12)
        
        # Credit utilization ratio (0-1)
        credit_utilization = np.random.beta(3, 4, n_users)
        
        # Savings account balance
        savings_balance = np.random.lognormal(10, 1.2, n_users).astype(int)
        savings_balance = np.clip(savings_balance, 5000, 500000)
        
        # Monthly savings rate (as % of income)
        monthly_savings_rate = np.random.beta(4, 6, n_users)
        
        # Number of bank accounts
        num_bank_accounts = np.random.choice([1, 2, 3, 4], n_users, p=[0.40, 0.35, 0.20, 0.05])
        
        # Insurance policies count
        insurance_policies = np.random.poisson(2, n_users)
        insurance_policies = np.clip(insurance_policies, 0, 6)
        
        # === Engineered Features ===
        # Debt to Income Ratio
        total_debt = existing_loans * incomes * np.random.uniform(0.1, 0.5, n_users)
        debt_to_income_ratio = total_debt / incomes
        debt_to_income_ratio = np.clip(debt_to_income_ratio, 0, 2)
        
        # Income to Spend Ratio
        total_monthly_spend = monthly_upi_spend + monthly_ecom_spend
        income_to_spend_ratio = incomes / (total_monthly_spend + 1)
        income_to_spend_ratio = np.clip(income_to_spend_ratio, 0.2, 10)
        
        # Savings Stability Index
        savings_stability_index = 1 / (1 + transaction_volatility)
        
        # Investment Maturity Score
        investment_maturity_score = (
            (avg_holding_period / 730) * 
            (1 - portfolio_risk) * 
            (1 + profit_loss_ratio) *
            (num_stocks / 50)
        )
        investment_maturity_score = np.clip(investment_maturity_score, 0, 1)
        
        # Financial Health Score
        financial_health_score = (
            (monthly_savings_rate * 0.3) +
            ((1 - credit_utilization) * 0.3) +
            ((1 - debt_to_income_ratio/2) * 0.2) +
            (investment_maturity_score * 0.2)
        )
        financial_health_score = np.clip(financial_health_score, 0, 1)
        
        # === Target Variable: Default Flag ===
        # Create a realistic default probability based on multiple factors
        default_prob = (
            0.05 +  # Base default rate
            0.20 * (1 - incomes / incomes.max()) +  # Lower income = higher risk
            0.15 * (debt_to_income_ratio / 2) +  # Higher debt = higher risk
            0.10 * (late_payment_count / 12) +  # Late payments = higher risk
            0.10 * credit_utilization +  # High credit utilization = higher risk
            0.10 * (1 - financial_health_score) +  # Poor financial health = higher risk
            0.10 * (existing_loans / 5) -  # More loans = higher risk
            0.10 * (credit_history_length / 240) -  # Longer history = lower risk
            0.10 * investment_maturity_score  # Good investments = lower risk
        )
        
        default_prob = np.clip(default_prob, 0.01, 0.95)
        default_flag = np.random.binomial(1, default_prob, n_users)
        
        # Create comprehensive dataframe
        data = pd.DataFrame({
            'user_id': user_ids,
            'age': ages,
            'income': incomes,
            'city': cities,
            'occupation': occupations,
            'education': education,
            'employment_length': employment_length,
            'existing_loans': existing_loans,
            'credit_history_length': credit_history_length,
            'upi_transaction_count': upi_transaction_count,
            'avg_upi_amount': avg_upi_amount,
            'monthly_upi_spend': monthly_upi_spend,
            'transaction_volatility': transaction_volatility,
            'income_ratio': income_ratio,
            'merchant_diversity': merchant_diversity,
            'monthly_ecom_spend': monthly_ecom_spend,
            'avg_cart_value': avg_cart_value,
            'num_orders': num_orders,
            'return_rate': return_rate,
            'category_diversity': category_diversity,
            'payment_preference': payment_preference,
            'num_stocks': num_stocks,
            'total_investment': total_investment,
            'portfolio_risk': portfolio_risk,
            'avg_holding_period': avg_holding_period,
            'profit_loss_ratio': profit_loss_ratio,
            'sip_active': sip_active,
            'late_payment_count': late_payment_count,
            'credit_utilization': credit_utilization,
            'savings_balance': savings_balance,
            'monthly_savings_rate': monthly_savings_rate,
            'num_bank_accounts': num_bank_accounts,
            'insurance_policies': insurance_policies,
            'debt_to_income_ratio': debt_to_income_ratio,
            'income_to_spend_ratio': income_to_spend_ratio,
            'savings_stability_index': savings_stability_index,
            'investment_maturity_score': investment_maturity_score,
            'financial_health_score': financial_health_score,
            'default_flag': default_flag
        })
        
        print(f"Dataset created with {len(data)} users")
        print(f"Default rate: {default_flag.mean()*100:.2f}%")
        
        return data
    
    def preprocess_data(self, data):
        """Preprocess data for modeling"""
        print("\nPreprocessing data...")
        
        # Separate features and target
        X = data.drop(['user_id', 'default_flag'], axis=1)
        y = data['default_flag']
        
        # Identify categorical and numerical features
        categorical_features = ['city', 'occupation', 'education', 'payment_preference']
        numerical_features = [col for col in X.columns if col not in categorical_features]
        
        # Encode categorical features
        X_encoded = X.copy()
        for col in categorical_features:
            le = LabelEncoder()
            X_encoded[col] = le.fit_transform(X[col])
            self.label_encoders[col] = le
        
        # Store feature names
        self.feature_names = X_encoded.columns.tolist()
        
        return X_encoded, y
    
    def perform_eda(self, data):
        """Perform exploratory data analysis"""
        print("\nPerforming Exploratory Data Analysis...")
        
        # Create figure with subplots
        fig, axes = plt.subplots(3, 3, figsize=(20, 16))
        fig.suptitle('Credit Risk Analysis - Exploratory Data Analysis', fontsize=16, fontweight='bold')
        
        # Plot 1: Default distribution
        default_counts = data['default_flag'].value_counts()
        axes[0, 0].pie(default_counts.values, labels=['No Default', 'Default'], autopct='%1.1f%%', colors=['#90EE90', '#FFB6C6'])
        axes[0, 0].set_title('Default Distribution')
        
        # Plot 2: Income distribution by default status
        sns.histplot(data=data, x='income', hue='default_flag', kde=True, bins=40, ax=axes[0, 1])
        axes[0, 1].set_title('Income Distribution by Default Status')
        axes[0, 1].set_xlabel('Income (INR)')
        
        # Plot 3: Age distribution
        sns.histplot(data=data, x='age', hue='default_flag', kde=True, bins=30, ax=axes[0, 2])
        axes[0, 2].set_title('Age Distribution by Default Status')
        
        # Plot 4: Debt to Income Ratio
        sns.boxplot(data=data, x='default_flag', y='debt_to_income_ratio', ax=axes[1, 0])
        axes[1, 0].set_title('Debt to Income Ratio by Default Status')
        axes[1, 0].set_xticklabels(['No Default', 'Default'])
        
        # Plot 5: Financial Health Score
        sns.violinplot(data=data, x='default_flag', y='financial_health_score', ax=axes[1, 1])
        axes[1, 1].set_title('Financial Health Score by Default Status')
        axes[1, 1].set_xticklabels(['No Default', 'Default'])
        
        # Plot 6: Credit Utilization
        sns.boxplot(data=data, x='default_flag', y='credit_utilization', ax=axes[1, 2])
        axes[1, 2].set_title('Credit Utilization by Default Status')
        axes[1, 2].set_xticklabels(['No Default', 'Default'])
        
        # Plot 7: Existing Loans
        loan_default = data.groupby('existing_loans')['default_flag'].mean()
        axes[2, 0].bar(loan_default.index, loan_default.values, color='coral')
        axes[2, 0].set_title('Default Rate by Number of Existing Loans')
        axes[2, 0].set_xlabel('Number of Existing Loans')
        axes[2, 0].set_ylabel('Default Rate')
        
        # Plot 8: Occupation-wise default rate
        occupation_default = data.groupby('occupation')['default_flag'].mean().sort_values()
        axes[2, 1].barh(occupation_default.index, occupation_default.values, color='skyblue')
        axes[2, 1].set_title('Default Rate by Occupation')
        axes[2, 1].set_xlabel('Default Rate')
        
        # Plot 9: Top correlations with default
        correlations = data.select_dtypes(include=[np.number]).corr()['default_flag'].sort_values(ascending=False)
        top_corr = correlations.drop('default_flag').head(10)
        top_corr.plot(kind='barh', ax=axes[2, 2], color='teal')
        axes[2, 2].set_title('Top 10 Features Correlated with Default')
        axes[2, 2].set_xlabel('Correlation Coefficient')
        
        plt.tight_layout()
        plt.savefig('comprehensive_eda.png', dpi=300, bbox_inches='tight')
        print("EDA plots saved as 'comprehensive_eda.png'")
        plt.close()
    
    def train_models(self, X, y):
        """Train and compare multiple models"""
        print("\nTraining models...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        
        # Handle class imbalance with SMOTE
        print("Applying SMOTE to handle class imbalance...")
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
        
        print(f"Original training set shape: {X_train.shape}")
        print(f"Resampled training set shape: {X_train_resampled.shape}")
        print(f"Original default rate: {y_train.mean()*100:.2f}%")
        print(f"Resampled default rate: {y_train_resampled.mean()*100:.2f}%")
        
        # Scale features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train_resampled)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Define models with hyperparameters
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', max_depth=15),
            'XGBoost': XGBClassifier(
                random_state=42, 
                eval_metric='logloss',
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                scale_pos_weight=len(y_train_resampled[y_train_resampled==0])/len(y_train_resampled[y_train_resampled==1])
            ),
            'LightGBM': LGBMClassifier(
                random_state=42,
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                class_weight='balanced',
                verbose=-1
            ),
            'CatBoost': CatBoostClassifier(
                random_state=42,
                iterations=100,
                depth=6,
                learning_rate=0.1,
                verbose=False,
                class_weights=[1, len(y_train_resampled[y_train_resampled==0])/len(y_train_resampled[y_train_resampled==1])]
            ),
            'Neural Network': MLPClassifier(
                hidden_layer_sizes=(100, 50),
                max_iter=1000,
                random_state=42,
                alpha=0.01,
                learning_rate='adaptive'
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            'Balanced Random Forest': BalancedRandomForestClassifier(
                n_estimators=100,
                random_state=42,
                max_depth=15,
                sampling_strategy='auto',
                replacement=True
            ),
            'Support Vector Machine': SVC(
                probability=True,
                random_state=42,
                class_weight='balanced',
                kernel='rbf',
                C=1.0
            ),
            'K-Nearest Neighbors': KNeighborsClassifier(
                n_neighbors=5,
                weights='distance'
            ),
            'Decision Tree': DecisionTreeClassifier(
                random_state=42,
                class_weight='balanced',
                max_depth=15
            ),
            'Gaussian Naive Bayes': GaussianNB()
        }
        
        # Create ensemble models
        print("\nCreating ensemble models...")
        
        # Voting Classifier (Soft Voting)
        voting_estimators = [
            ('rf', models['Random Forest']),
            ('xgb', models['XGBoost']),
            ('lgbm', models['LightGBM']),
            ('nn', models['Neural Network'])
        ]
        models['Voting Classifier'] = VotingClassifier(
            estimators=voting_estimators,
            voting='soft'
        )
        
        # Stacking Classifier
        base_estimators = [
            ('rf', models['Random Forest']),
            ('xgb', models['XGBoost']),
            ('lgbm', models['LightGBM'])
        ]
        models['Stacking Classifier'] = StackingClassifier(
            estimators=base_estimators,
            final_estimator=LogisticRegression(max_iter=1000, random_state=42),
            cv=5
        )
        
        # Train and evaluate models
        results = {}
        model_names = []
        training_times = []
        
        print("\n" + "="*80)
        print("MODEL TRAINING AND EVALUATION")
        print("="*80)
        
        for name, model in models.items():
            print(f"\nTraining {name}...")
            start_time = time.time()
            
            # Train model
            model.fit(X_train_scaled, y_train_resampled)
            
            training_time = time.time() - start_time
            training_times.append(training_time)
            model_names.append(name)
            
            # Predictions
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            # Metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            roc_auc = roc_auc_score(y_test, y_pred_proba)
            
            # Cross-validation score
            cv_scores = cross_val_score(model, X_train_scaled, y_train_resampled, cv=5, scoring='roc_auc')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            results[name] = {
                'model': model,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'training_time': training_time,
                'predictions': y_pred_proba
            }
            
            print(f"  Accuracy: {accuracy:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            print(f"  F1-Score: {f1:.4f}")
            print(f"  ROC-AUC: {roc_auc:.4f}")
            print(f"  CV Score: {cv_mean:.4f} (+/- {cv_std:.4f})")
            print(f"  Training Time: {training_time:.2f} seconds")
        
        # Create model comparison dataframe
        self.model_comparison = pd.DataFrame({
            'Model': model_names,
            'Accuracy': [results[name]['accuracy'] for name in model_names],
            'Precision': [results[name]['precision'] for name in model_names],
            'Recall': [results[name]['recall'] for name in model_names],
            'F1-Score': [results[name]['f1'] for name in model_names],
            'ROC-AUC': [results[name]['roc_auc'] for name in model_names],
            'CV Score': [results[name]['cv_mean'] for name in model_names],
            'CV Std': [results[name]['cv_std'] for name in model_names],
            'Training Time (s)': [results[name]['training_time'] for name in model_names]
        })
        
        # Sort by ROC-AUC
        self.model_comparison = self.model_comparison.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)
        
        # Display model comparison
        print("\n" + "="*80)
        print("MODEL COMPARISON TABLE")
        print("="*80)
        print(self.model_comparison.to_string(index=False, float_format='%.4f'))
        
        # Find best model based on ROC-AUC
        self.best_model_name = self.model_comparison.iloc[0]['Model']
        self.model = results[self.best_model_name]['model']
        
        print(f"\n{'='*80}")
        print(f"BEST MODEL: {self.best_model_name}")
        print(f"ROC-AUC: {self.model_comparison.iloc[0]['ROC-AUC']:.4f}")
        print(f"Accuracy: {self.model_comparison.iloc[0]['Accuracy']:.4f}")
        print(f"F1-Score: {self.model_comparison.iloc[0]['F1-Score']:.4f}")
        print(f"{'='*80}")
        
        # Create visualizations
        self.create_model_visualizations(results, X_test_scaled, y_test)
        
        # Extract feature importance
        self.extract_feature_importance(X)
        
        # Save model
        joblib.dump(self.model, 'best_model.pkl')
        joblib.dump(self.scaler, 'scaler.pkl')
        joblib.dump(self.label_encoders, 'label_encoders.pkl')
        joblib.dump(self.feature_names, 'feature_names.pkl')
        joblib.dump(self.model_comparison, 'model_comparison.pkl')
        
        return results, X_test_scaled, y_test
    
    def create_model_visualizations(self, results, X_test, y_test):
        """Create comprehensive model performance visualizations"""
        print("\nCreating model performance visualizations...")
        
        # 1. Model Performance Comparison
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
        
        # ROC-AUC Comparison
        roc_auc_values = [results[name]['roc_auc'] for name in results.keys()]
        model_names = list(results.keys())
        
        axes[0, 0].barh(model_names, roc_auc_values, color='skyblue')
        axes[0, 0].set_title('ROC-AUC Score Comparison')
        axes[0, 0].set_xlabel('ROC-AUC Score')
        axes[0, 0].set_xlim(0, 1)
        
        # Add value labels
        for i, v in enumerate(roc_auc_values):
            axes[0, 0].text(v + 0.01, i, f"{v:.4f}", va='center')
        
        # Accuracy Comparison
        accuracy_values = [results[name]['accuracy'] for name in results.keys()]
        axes[0, 1].barh(model_names, accuracy_values, color='lightgreen')
        axes[0, 1].set_title('Accuracy Comparison')
        axes[0, 1].set_xlabel('Accuracy Score')
        axes[0, 1].set_xlim(0, 1)
        
        # Add value labels
        for i, v in enumerate(accuracy_values):
            axes[0, 1].text(v + 0.01, i, f"{v:.4f}", va='center')
        
        # F1-Score Comparison
        f1_values = [results[name]['f1'] for name in results.keys()]
        axes[1, 0].barh(model_names, f1_values, color='salmon')
        axes[1, 0].set_title('F1-Score Comparison')
        axes[1, 0].set_xlabel('F1-Score')
        axes[1, 0].set_xlim(0, 1)
        
        # Add value labels
        for i, v in enumerate(f1_values):
            axes[1, 0].text(v + 0.01, i, f"{v:.4f}", va='center')
        
        # Training Time Comparison
        training_times = [results[name]['training_time'] for name in results.keys()]
        axes[1, 1].barh(model_names, training_times, color='plum')
        axes[1, 1].set_title('Training Time Comparison')
        axes[1, 1].set_xlabel('Training Time (seconds)')
        
        # Add value labels
        for i, v in enumerate(training_times):
            axes[1, 1].text(v + max(training_times)*0.01, i, f"{v:.2f}s", va='center')
        
        plt.tight_layout()
        plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ROC Curves for Top Models
        plt.figure(figsize=(12, 10))
        
        # Plot ROC curves for top 5 models
        top_models = self.model_comparison.head(5)['Model'].tolist()
        
        for name in top_models:
            y_pred_proba = results[name]['predictions']
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            auc = roc_auc_score(y_test, y_pred_proba)
            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})')
        
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves for Top 5 Models')
        plt.legend()
        plt.grid(True)
        plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Precision-Recall Curves for Top Models
        plt.figure(figsize=(12, 10))
        
        for name in top_models:
            y_pred_proba = results[name]['predictions']
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            plt.plot(recall, precision, label=f'{name}')
        
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curves for Top 5 Models')
        plt.legend()
        plt.grid(True)
        plt.savefig('precision_recall_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. Confusion Matrix for Best Model
        y_pred = self.model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
        plt.title(f'Confusion Matrix - {self.best_model_name}')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("Model performance visualizations saved successfully!")
    
    def extract_feature_importance(self, X):
        """Extract and visualize feature importance from the best model"""
        print("\nExtracting feature importance...")
        
        feature_names = X.columns.tolist()
        
        # Check if the model has feature_importances_ attribute
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
            # Create dataframe for feature importance
            self.feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values('Importance', ascending=False)
            
            # Plot feature importance
            plt.figure(figsize=(12, 10))
            top_features = self.feature_importance_df.head(20)
            plt.barh(top_features['Feature'], top_features['Importance'])
            plt.xlabel('Feature Importance')
            plt.title(f'Top 20 Feature Importance - {self.best_model_name}')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print("Feature importance extracted and saved successfully!")
        else:
            print(f"Model {self.best_model_name} does not have feature_importances_ attribute")
            self.feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': [0] * len(feature_names)
            })
    
    def plot_confusion_matrix(self, y_test, y_pred):
        """Plot confusion matrix"""
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
        plt.title('Confusion Matrix')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("Confusion matrix saved as 'confusion_matrix.png'")
    
    def run_pipeline(self):
        """Run the complete pipeline"""
        print("=" * 80)
        print("NEXT-GEN CREDIT RISK MODELLING PIPELINE")
        print("=" * 80)
        
        # Create dataset
        self.data = self.create_comprehensive_dataset()
        
        # Save processed data
        self.data.to_csv('credit_risk_data.csv', index=False)
        print("\nDataset saved as 'credit_risk_data.csv'")
        
        # Perform EDA
        self.perform_eda(self.data)
        
        # Preprocess data
        X, y = self.preprocess_data(self.data)
        
        # Train models
        results, X_test, y_test = self.train_models(X, y)
        
        print("\n" + "=" * 80)
        print("PIPELINE COMPLETED SUCCESSFULLY!")
        print("Files generated:")
        print("  - credit_risk_data.csv")
        print("  - comprehensive_eda.png")
        print("  - model_performance_comparison.png")
        print("  - roc_curves.png")
        print("  - precision_recall_curves.png")
        print("  - confusion_matrix.png")
        print("  - feature_importance.png")
        print("  - best_model.pkl")
        print("  - scaler.pkl")
        print("  - label_encoders.pkl")
        print("  - feature_names.pkl")
        print("  - model_comparison.pkl")
        print("=" * 80)
        
        return results

class StreamlitApp:
    """Streamlit web application for credit risk assessment"""
    
    def __init__(self):
        # Check if model files exist, if not run pipeline first
        if not os.path.exists('best_model.pkl'):
            print("Model files not found. Running pipeline first...")
            model = CreditRiskModel()
            model.run_pipeline()
        
        self.model = joblib.load('best_model.pkl')
        self.scaler = joblib.load('scaler.pkl')
        self.label_encoders = joblib.load('label_encoders.pkl')
        self.feature_names = joblib.load('feature_names.pkl')
        
        # Load model comparison if available
        if os.path.exists('model_comparison.pkl'):
            self.model_comparison = joblib.load('model_comparison.pkl')
        else:
            self.model_comparison = None
    
    def run(self):
        """Run the Streamlit app"""
        # Set page config
        st.set_page_config(
            page_title="Credit Risk Assessment",
            page_icon="üí≥",
            layout="wide",
            initial_sidebar_state="collapsed"
        )
        
        # Custom CSS
        st.markdown("""
            <style>
            .main-header {
                font-size: 42px;
                font-weight: bold;
                color: #1F77B4;
                text-align: center;
                padding: 20px;
                background: linear-gradient(90deg, #E3F2FD 0%, #BBDEFB 100%);
                border-radius: 10px;
                margin-bottom: 30px;
            }
            .sub-header {
                font-size: 24px;
                font-weight: bold;
                color: #2E7D32;
                margin-top: 20px;
                margin-bottom: 15px;
                padding: 10px;
                background-color: #E8F5E9;
                border-left: 5px solid #4CAF50;
            }
            .metric-card {
                background-color: #F5F5F5;
                padding: 20px;
                border-radius: 10px;
                box-shadow: 2px 2px 10px rgba(0,0,0,0.1);
            }
            .highlight {
                background-color: #FFF9C4;
                padding: 10px;
                border-radius: 5px;
                border-left: 3px solid #FBC02D;
            }
            </style>
        """, unsafe_allow_html=True)
        
        # App title
        st.markdown('<div class="main-header">üí≥ Next-Gen Credit Risk Assessment Platform</div>', unsafe_allow_html=True)
        
        st.markdown("""
        <div style="text-align: center; font-size: 18px; color: #555; margin-bottom: 30px;">
        Powered by Advanced Machine Learning | Using Alternative Financial Data Sources
        </div>
        """, unsafe_allow_html=True)
        
        # Create tabs for better organization
        tab1, tab2, tab3, tab4 = st.tabs(["üìä Risk Assessment", "üìà Model Performance", "üîç Feature Analysis", "‚ÑπÔ∏è About"])
        
        with tab1:
            st.markdown('<div class="sub-header">üë§ Personal & Professional Information</div>', unsafe_allow_html=True)
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                age = st.slider("Age", 21, 65, 35, help="Your current age")
                income = st.slider("Monthly Income (‚Çπ)", 15000, 300000, 50000, step=5000, help="Your monthly income in INR")
            
            with col2:
                city = st.selectbox("City", ["Mumbai", "Delhi", "Bangalore", "Chennai", "Kolkata", "Hyderabad", "Pune", "Ahmedabad"])
                occupation = st.selectbox("Occupation", ["Salaried", "Self-employed", "Business", "Freelancer", "Professional"])
            
            with col3:
                education = st.selectbox("Education Level", ["Graduate", "Post-Graduate", "Undergraduate", "Doctorate"])
                employment_length = st.slider("Employment Length (years)", 0, 40, 5)
            
            with col4:
                existing_loans = st.slider("Existing Loans", 0, 5, 1)
                credit_history_length = st.slider("Credit History (months)", 6, 240, 60)
            
            st.markdown('<div class="sub-header">üí≥ UPI Transaction Behavior</div>', unsafe_allow_html=True)
            
            col5, col6, col7, col8 = st.columns(4)
            
            with col5:
                upi_transaction_count = st.slider("Monthly UPI Transactions", 5, 200, 30)
                avg_upi_amount = st.slider("Avg UPI Amount (‚Çπ)", 100, 15000, 1000)
            
            with col6:
                monthly_upi_spend = st.slider("Monthly UPI Spend (‚Çπ)", 500, 100000, 15000, step=1000)
                transaction_volatility = st.slider("Transaction Volatility", 0.0, 1.0, 0.3, step=0.05)
            
            with col7:
                income_ratio = st.slider("Income Ratio (Incoming/Total)", 0.0, 1.0, 0.4, step=0.05)
                merchant_diversity = st.slider("Merchant Diversity (1-10)", 1, 10, 5)
            
            with col8:
                st.write("")  # Spacing
            
            st.markdown('<div class="sub-header">üõí E-commerce Spending Patterns</div>', unsafe_allow_html=True)
            
            col9, col10, col11, col12 = st.columns(4)
            
            with col9:
                monthly_ecom_spend = st.slider("Monthly E-commerce Spend (‚Çπ)", 1000, 80000, 10000, step=1000)
                avg_cart_value = st.slider("Average Cart Value (‚Çπ)", 500, 25000, 3000, step=500)
            
            with col10:
                num_orders = st.slider("Monthly Orders", 1, 40, 5)
                return_rate = st.slider("Return Rate", 0.0, 0.5, 0.1, step=0.05)
            
            with col11:
                category_diversity = st.slider("Category Diversity (1-8)", 1, 8, 4)
                payment_preference = st.selectbox("Payment Preference", ["UPI", "Credit Card", "Debit Card", "COD", "EMI"])
            
            with col12:
                st.write("")  # Spacing
            
            st.markdown('<div class="sub-header">üìà Investment Portfolio</div>', unsafe_allow_html=True)
            
            col13, col14, col15, col16 = st.columns(4)
            
            with col13:
                num_stocks = st.slider("Number of Stocks/MFs", 0, 50, 6)
                total_investment = st.slider("Total Investment (‚Çπ)", 0, 2000000, 100000, step=10000)
            
            with col14:
                portfolio_risk = st.slider("Portfolio Risk (0-1)", 0.0, 1.0, 0.5, step=0.05)
                avg_holding_period = st.slider("Avg Holding Period (days)", 1, 730, 180)
            
            with col15:
                profit_loss_ratio = st.slider("Profit/Loss Ratio", -0.6, 0.8, 0.12, step=0.05)
                sip_active = st.selectbox("SIP Active", [0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
            
            with col16:
                st.write("")  # Spacing
            
            st.markdown('<div class="sub-header">üè¶ Financial Health Indicators</div>', unsafe_allow_html=True)
            
            col17, col18, col19, col20 = st.columns(4)
            
            with col17:
                late_payment_count = st.slider("Late Payments (last 12 months)", 0, 12, 1)
                credit_utilization = st.slider("Credit Utilization", 0.0, 1.0, 0.4, step=0.05)
            
            with col18:
                savings_balance = st.slider("Savings Balance (‚Çπ)", 5000, 500000, 50000, step=5000)
                monthly_savings_rate = st.slider("Monthly Savings Rate", 0.0, 1.0, 0.2, step=0.05)
            
            with col19:
                num_bank_accounts = st.selectbox("Number of Bank Accounts", [1, 2, 3, 4])
                insurance_policies = st.slider("Insurance Policies", 0, 6, 2)
            
            with col20:
                st.write("")  # Spacing
            
            # Calculate engineered features
            total_debt = existing_loans * income * 0.3
            debt_to_income_ratio = total_debt / income if income > 0 else 0
            debt_to_income_ratio = min(debt_to_income_ratio, 2)
            
            total_monthly_spend = monthly_upi_spend + monthly_ecom_spend
            income_to_spend_ratio = income / (total_monthly_spend + 1)
            income_to_spend_ratio = min(max(income_to_spend_ratio, 0.2), 10)
            
            savings_stability_index = 1 / (1 + transaction_volatility)
            
            investment_maturity_score = (
                (avg_holding_period / 730) * 
                (1 - portfolio_risk) * 
                (1 + profit_loss_ratio) *
                (num_stocks / 50)
            )
            investment_maturity_score = min(max(investment_maturity_score, 0), 1)
            
            financial_health_score = (
                (monthly_savings_rate * 0.3) +
                ((1 - credit_utilization) * 0.3) +
                ((1 - debt_to_income_ratio/2) * 0.2) +
                (investment_maturity_score * 0.2)
            )
            financial_health_score = min(max(financial_health_score, 0), 1)
            
            # Center the assess button
            st.markdown("<br>", unsafe_allow_html=True)
            col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 1])
            with col_btn2:
                assess_button = st.button("üöÄ Assess Credit Risk", type="primary", use_container_width=True)
            
            if assess_button:
                # Create input dataframe
                input_data = pd.DataFrame({
                    'age': [age],
                    'income': [income],
                    'city': [city],
                    'occupation': [occupation],
                    'education': [education],
                    'employment_length': [employment_length],
                    'existing_loans': [existing_loans],
                    'credit_history_length': [credit_history_length],
                    'upi_transaction_count': [upi_transaction_count],
                    'avg_upi_amount': [avg_upi_amount],
                    'monthly_upi_spend': [monthly_upi_spend],
                    'transaction_volatility': [transaction_volatility],
                    'income_ratio': [income_ratio],
                    'merchant_diversity': [merchant_diversity],
                    'monthly_ecom_spend': [monthly_ecom_spend],
                    'avg_cart_value': [avg_cart_value],
                    'num_orders': [num_orders],
                    'return_rate': [return_rate],
                    'category_diversity': [category_diversity],
                    'payment_preference': [payment_preference],
                    'num_stocks': [num_stocks],
                    'total_investment': [total_investment],
                    'portfolio_risk': [portfolio_risk],
                    'avg_holding_period': [avg_holding_period],
                    'profit_loss_ratio': [profit_loss_ratio],
                    'sip_active': [sip_active],
                    'late_payment_count': [late_payment_count],
                    'credit_utilization': [credit_utilization],
                    'savings_balance': [savings_balance],
                    'monthly_savings_rate': [monthly_savings_rate],
                    'num_bank_accounts': [num_bank_accounts],
                    'insurance_policies': [insurance_policies],
                    'debt_to_income_ratio': [debt_to_income_ratio],
                    'income_to_spend_ratio': [income_to_spend_ratio],
                    'savings_stability_index': [savings_stability_index],
                    'investment_maturity_score': [investment_maturity_score],
                    'financial_health_score': [financial_health_score]
                })
                
                # Encode categorical features
                for col in ['city', 'occupation', 'education', 'payment_preference']:
                    input_data[col] = self.label_encoders[col].transform(input_data[col])
                
                # Ensure columns are in the same order as training
                input_data = input_data[self.feature_names]
                
                # Scale features
                input_scaled = self.scaler.transform(input_data)
                
                # Make prediction
                prediction_proba = self.model.predict_proba(input_scaled)[0, 1]
                prediction = self.model.predict(input_scaled)[0]
                
                # Display results
                st.markdown("<br>", unsafe_allow_html=True)
                st.markdown('<div class="sub-header">üìä Credit Risk Assessment Results</div>', unsafe_allow_html=True)
                
                # Create columns for results
                result_col1, result_col2, result_col3 = st.columns([1, 2, 1])
                
                with result_col2:
                    # Create gauge chart
                    fig = go.Figure(go.Indicator(
                        mode = "gauge+number+delta",
                        value = prediction_proba * 100,
                        domain = {'x': [0, 1], 'y': [0, 1]},
                        title = {'text': "Credit Risk Score (%)", 'font': {'size': 24}},
                        delta = {'reference': 50, 'increasing': {'color': "red"}, 'decreasing': {'color': "green"}},
                        gauge = {
                            'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': "darkblue"},
                            'bar': {'color': "darkblue"},
                            'bgcolor': "white",
                            'borderwidth': 2,
                            'bordercolor': "gray",
                            'steps': [
                                {'range': [0, 20], 'color': '#90EE90'},
                                {'range': [20, 40], 'color': '#98FB98'},
                                {'range': [40, 60], 'color': '#FFEB3B'},
                                {'range': [60, 80], 'color': '#FFB347'},
                                {'range': [80, 100], 'color': '#FF6961'}
                            ],
                            'threshold': {
                                'line': {'color': "red", 'width': 4},
                                'thickness': 0.75,
                                'value': 90
                            }
                        }
                    ))
                    
                    fig.update_layout(
                        height=400,
                        font = {'color': "darkblue", 'family': "Arial"}
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Risk categorization
                if prediction_proba < 0.2:
                    risk_category = "Excellent"
                    risk_color = "#4CAF50"
                    risk_emoji = "‚úÖ"
                    risk_message = "Very Low Credit Risk - Highly Creditworthy"
                elif prediction_proba < 0.4:
                    risk_category = "Good"
                    risk_color = "#8BC34A"
                    risk_emoji = "üëç"
                    risk_message = "Low Credit Risk - Creditworthy"
                elif prediction_proba < 0.6:
                    risk_category = "Fair"
                    risk_color = "#FFC107"
                    risk_emoji = "‚ö†Ô∏è"
                    risk_message = "Moderate Credit Risk - Review Required"
                elif prediction_proba < 0.8:
                    risk_category = "Poor"
                    risk_color = "#FF9800"
                    risk_emoji = "‚ö°"
                    risk_message = "High Credit Risk - Careful Review Needed"
                else:
                    risk_category = "Very Poor"
                    risk_color = "#F44336"
                    risk_emoji = "‚ùå"
                    risk_message = "Very High Credit Risk - Not Recommended"
                
                st.markdown(f"""
                    <div style="text-align: center; padding: 20px; background-color: {risk_color}20; border-radius: 10px; border: 2px solid {risk_color};">
                        <h2 style="color: {risk_color}; margin: 0;">{risk_emoji} Risk Category: {risk_category}</h2>
                        <p style="font-size: 18px; color: #555; margin-top: 10px;">{risk_message}</p>
                    </div>
                """, unsafe_allow_html=True)
                
                st.markdown("<br>", unsafe_allow_html=True)
                
                # Display key metrics
                st.markdown('<div class="sub-header">üìå Key Financial Metrics</div>', unsafe_allow_html=True)
                
                metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
                
                with metric_col1:
                    fhs_color = "green" if financial_health_score > 0.6 else "orange" if financial_health_score > 0.4 else "red"
                    st.markdown(f"""
                        <div class="metric-card">
                            <h4 style="color: #555; margin: 0;">Financial Health Score</h4>
                            <h2 style="color: {fhs_color}; margin: 10px 0;">{financial_health_score:.2f}</h2>
                        </div>
                    """, unsafe_allow_html=True)
                
                with metric_col2:
                    dti_color = "green" if debt_to_income_ratio < 0.3 else "orange" if debt_to_income_ratio < 0.5 else "red"
                    st.markdown(f"""
                        <div class="metric-card">
                            <h4 style="color: #555; margin: 0;">Debt-to-Income Ratio</h4>
                            <h2 style="color: {dti_color}; margin: 10px 0;">{debt_to_income_ratio:.2f}</h2>
                        </div>
                    """, unsafe_allow_html=True)
                
                with metric_col3:
                    ims_color = "green" if investment_maturity_score > 0.6 else "orange" if investment_maturity_score > 0.3 else "red"
                    st.markdown(f"""
                        <div class="metric-card">
                            <h4 style="color: #555; margin: 0;">Investment Maturity</h4>
                            <h2 style="color: {ims_color}; margin: 10px 0;">{investment_maturity_score:.2f}</h2>
                        </div>
                    """, unsafe_allow_html=True)
                
                with metric_col4:
                    cu_color = "green" if credit_utilization < 0.3 else "orange" if credit_utilization < 0.6 else "red"
                    st.markdown(f"""
                        <div class="metric-card">
                            <h4 style="color: #555; margin: 0;">Credit Utilization</h4>
                            <h2 style="color: {cu_color}; margin: 10px 0;">{credit_utilization:.2f}</h2>
                        </div>
                    """, unsafe_allow_html=True)
                
                st.markdown("<br>", unsafe_allow_html=True)
                
                # Recommendations
                st.markdown('<div class="sub-header">üí° Personalized Recommendations</div>', unsafe_allow_html=True)
                
                recommendations = []
                
                if debt_to_income_ratio > 0.4:
                    recommendations.append("üè¶ **Reduce Debt**: Your debt-to-income ratio is high. Focus on paying down existing loans.")
                
                if credit_utilization > 0.5:
                    recommendations.append("üí≥ **Lower Credit Utilization**: Try to keep your credit utilization below 30% for better scores.")
                
                if late_payment_count > 2:
                    recommendations.append("‚è∞ **Improve Payment History**: Set up auto-payments to avoid late payments.")
                
                if monthly_savings_rate < 0.15:
                    recommendations.append("üí∞ **Increase Savings**: Aim to save at least 20% of your monthly income.")
                
                if investment_maturity_score < 0.3:
                    recommendations.append("üìà **Build Investment Portfolio**: Consider long-term investments for better financial stability.")
                
                if transaction_volatility > 0.6:
                    recommendations.append("üìä **Stabilize Spending**: Try to maintain consistent spending patterns month over month.")
                
                if sip_active == 0 and income > 30000:
                    recommendations.append("üéØ **Start SIP**: Consider starting a Systematic Investment Plan for disciplined investing.")
                
                if insurance_policies < 2:
                    recommendations.append("üõ°Ô∏è **Get Adequate Insurance**: Ensure you have sufficient life and health insurance coverage.")
                
                if len(recommendations) == 0:
                    st.success("üéâ **Excellent Financial Profile!** Keep maintaining your good financial habits.")
                else:
                    for rec in recommendations:
                        st.info(rec)
                
                # Action items
                st.markdown('<div class="sub-header">‚úÖ Immediate Action Items</div>', unsafe_allow_html=True)
                
                action_col1, action_col2 = st.columns(2)
                
                with action_col1:
                    st.markdown("""
                        **Short-term (1-3 months):**
                        - Review and reduce unnecessary expenses
                        - Set up emergency fund (3-6 months expenses)
                        - Pay all bills on time
                        - Check credit report for errors
                    """)
                
                with action_col2:
                    st.markdown("""
                        **Long-term (6-12 months):**
                        - Build diversified investment portfolio
                        - Reduce debt-to-income ratio below 30%
                        - Increase credit history length
                        - Maintain consistent savings habit
                    """)
        
        with tab2:
            st.markdown('<div class="sub-header">üìä Model Performance Analysis</div>', unsafe_allow_html=True)
            
            if self.model_comparison is not None:
                # Display model comparison table
                st.markdown("### üèÜ Model Ranking")
                
                # Create a copy for display
                display_df = self.model_comparison.copy()
                
                # Format numeric columns for better display
                display_df['ROC-AUC'] = display_df['ROC-AUC'].round(4)
                display_df['Accuracy'] = display_df['Accuracy'].round(4)
                display_df['Precision'] = display_df['Precision'].round(4)
                display_df['Recall'] = display_df['Recall'].round(4)
                display_df['F1-Score'] = display_df['F1-Score'].round(4)
                display_df['CV Score'] = display_df['CV Score'].round(4)
                display_df['CV Std'] = display_df['CV Std'].round(4)
                display_df['Training Time (s)'] = display_df['Training Time (s)'].round(2)
                
                # Combine CV Score and CV Std for display
                display_df['CV Score (¬±Std)'] = display_df.apply(
                    lambda row: f"{row['CV Score']:.4f} ¬± {row['CV Std']:.4f}", axis=1
                )
                
                # Select columns to display
                display_cols = ['Model', 'ROC-AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'CV Score (¬±Std)', 'Training Time (s)']
                display_df = display_df[display_cols]
                
                # Display the table with highlighting for the best model
                def highlight_best_model(s):
                    is_best = s.index == 0  # First row is the best model (sorted by ROC-AUC)
                    return ['background-color: #90EE90' if v else '' for v in is_best]
                
                styled_df = display_df.style.apply(highlight_best_model, axis=0)
                st.dataframe(styled_df, use_container_width=True)
                
                # Model performance visualizations
                st.markdown("### üìà Performance Visualizations")
                
                viz_col1, viz_col2 = st.columns(2)
                
                with viz_col1:
                    if os.path.exists('model_performance_comparison.png'):
                        st.image('model_performance_comparison.png', caption='Model Performance Comparison', use_column_width=True)
                
                with viz_col2:
                    if os.path.exists('roc_curves.png'):
                        st.image('roc_curves.png', caption='ROC Curves for Top Models', use_column_width=True)
                
                viz_col3, viz_col4 = st.columns(2)
                
                with viz_col3:
                    if os.path.exists('precision_recall_curves.png'):
                        st.image('precision_recall_curves.png', caption='Precision-Recall Curves', use_column_width=True)
                
                with viz_col4:
                    if os.path.exists('confusion_matrix.png'):
                        st.image('confusion_matrix.png', caption='Confusion Matrix', use_column_width=True)
                
                # Model insights
                st.markdown("### üîç Model Insights")
                
                best_model = self.model_comparison.iloc[0]
                st.markdown(f"""
                    <div class="highlight">
                        <h4>Best Performing Model: {best_model['Model']}</h4>
                        <p>The {best_model['Model']} achieved the highest ROC-AUC score of {best_model['ROC-AUC']:.4f}, 
                        indicating superior discriminative ability between default and non-default cases.</p>
                        <p>Key performance metrics:</p>
                        <ul>
                            <li>Accuracy: {best_model['Accuracy']:.4f}</li>
                            <li>Precision: {best_model['Precision']:.4f}</li>
                            <li>Recall: {best_model['Recall']:.4f}</li>
                            <li>F1-Score: {best_model['F1-Score']:.4f}</li>
                            <li>Cross-validation Score: {best_model['CV Score']:.4f}</li>
                        </ul>
                    </div>
                """, unsafe_allow_html=True)
                
                # Model comparison insights
                st.markdown("### üìä Comparative Analysis")
                
                # Create a comparison of top models
                top_models = self.model_comparison.head(3)
                
                fig = go.Figure()
                
                metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
                
                for metric in metrics:
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=top_models['Model'],
                        y=top_models[metric],
                        yaxis='y',
                    ))
                
                fig.update_layout(
                    title='Performance Metrics Comparison for Top 3 Models',
                    xaxis=dict(domain=[0, 1]),
                    yaxis=dict(
                        title=dict(
                            text='Score',
                            font=dict(size=16)
                        ),
                        tickfont=dict(size=14),
                    ),
                    legend=dict(
                        x=0.01,
                        y=0.99,
                        bgcolor='rgba(255, 255, 255, 0)',
                        bordercolor='rgba(255, 255, 255, 0)'
                    ),
                    barmode='group',
                    bargap=0.15,
                    bargroupgap=0.1
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Training time analysis
                st.markdown("### ‚è±Ô∏è Training Time Analysis")
                
                fig_time = go.Figure(data=[
                    go.Bar(
                        x=self.model_comparison['Model'],
                        y=self.model_comparison['Training Time (s)'],
                        text=self.model_comparison['Training Time (s)'].apply(lambda x: f"{x:.2f}s"),
                        textposition='auto',
                        marker_color='lightblue'
                    )
                ])
                
                fig_time.update_layout(
                    title='Model Training Time Comparison',
                    xaxis_title='Model',
                    yaxis_title='Training Time (seconds)',
                    height=500
                )
                
                st.plotly_chart(fig_time, use_container_width=True)
                
                # Performance vs. Complexity trade-off
                st.markdown("### ‚öñÔ∏è Performance vs. Complexity Trade-off")
                
                # Create a simple complexity score based on training time
                complexity_df = self.model_comparison.copy()
                complexity_df['Complexity'] = complexity_df['Training Time (s)'] / complexity_df['Training Time (s)'].max()
                
                fig_tradeoff = go.Figure()
                
                fig_tradeoff.add_trace(go.Scatter(
                    x=complexity_df['Complexity'],
                    y=complexity_df['ROC-AUC'],
                    mode='markers+text',
                    text=complexity_df['Model'],
                    textposition="top center",
                    marker=dict(
                        size=12,
                        color=complexity_df['ROC-AUC'],
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="ROC-AUC")
                    )
                ))
                
                fig_tradeoff.update_layout(
                    title='Performance vs. Complexity Trade-off',
                    xaxis_title='Model Complexity (normalized)',
                    yaxis_title='ROC-AUC Score',
                    height=500
                )
                
                st.plotly_chart(fig_tradeoff, use_container_width=True)
                
            else:
                st.warning("Model comparison data not found. Please run the pipeline first to generate model performance data.")
        
        with tab3:
            st.markdown('<div class="sub-header">üîç Feature Importance Analysis</div>', unsafe_allow_html=True)
            
            # Load the dataset if available
            if os.path.exists('credit_risk_data.csv'):
                data = pd.read_csv('credit_risk_data.csv')
                
                # Feature importance visualization
                if os.path.exists('feature_importance.png'):
                    st.image('feature_importance.png', caption='Feature Importance', use_column_width=True)
                
                # Feature correlation heatmap
                st.markdown("### üî• Feature Correlation Heatmap")
                
                numeric_cols = data.select_dtypes(include=[np.number]).columns[:15]
                corr_matrix = data[numeric_cols].corr()
                
                fig = px.imshow(
                    corr_matrix,
                    text_auto='.2f',
                    aspect="auto",
                    title="Correlation Matrix of Key Features",
                    color_continuous_scale='RdBu_r'
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Feature distribution by default status
                st.markdown("### üìä Feature Distribution by Default Status")
                
                # Select top features for visualization
                top_features = ['debt_to_income_ratio', 'financial_health_score', 'credit_utilization', 
                               'late_payment_count', 'investment_maturity_score']
                
                for feature in top_features:
                    fig = px.histogram(
                        data, 
                        x=feature, 
                        color='default_flag',
                        title=f'Distribution of {feature.replace("_", " ").title()} by Default Status',
                        nbins=50,
                        marginal='box'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Feature importance table
                if os.path.exists('feature_importance.pkl'):
                    feature_importance = joblib.load('feature_importance.pkl')
                    
                    st.markdown("### üìã Feature Importance Table")
                    
                    # Format the importance values
                    feature_importance['Importance'] = feature_importance['Importance'].apply(lambda x: f"{x:.6f}")
                    feature_importance['Percentage'] = (feature_importance['Importance'].astype(float) * 100).apply(lambda x: f"{x:.2f}%")
                    
                    st.dataframe(feature_importance.head(20), use_container_width=True)
            else:
                st.warning("Dataset not found. Please run the pipeline first to generate the dataset.")
        
        with tab4:
            st.markdown('<div class="sub-header">‚ÑπÔ∏è About This Platform</div>', unsafe_allow_html=True)
            
            st.markdown("""
            ### üéØ Overview
            
            This **Next-Gen Credit Risk Assessment Platform** uses advanced machine learning algorithms to evaluate 
            creditworthiness based on alternative financial data sources. Unlike traditional credit scoring systems 
            that rely heavily on credit bureau data, our platform analyzes:
            
            - **UPI Transaction Behavior**: Spending patterns, transaction frequency, merchant diversity
            - **E-commerce Activity**: Purchase history, cart values, return rates, category preferences
            - **Investment Portfolio**: Stock holdings, mutual funds, SIPs, risk appetite, returns
            - **Financial Health Indicators**: Debt ratios, savings rate, credit utilization, payment history
            
            ### ü§ñ Machine Learning Models
            
            We employ multiple state-of-the-art machine learning algorithms:
            
            1. **Logistic Regression**: For interpretable baseline predictions
            2. **Random Forest**: For capturing non-linear relationships and feature interactions
            3. **XGBoost**: For superior predictive performance with gradient boosting
            4. **LightGBM**: Fast gradient boosting with leaf-wise growth
            5. **CatBoost**: Gradient boosting with categorical feature support
            6. **Neural Network**: Multi-layer perceptron for complex pattern recognition
            7. **Gradient Boosting**: Ensemble method for predictive accuracy
            8. **Balanced Random Forest**: Random forest with balanced class weights
            9. **Support Vector Machine**: For high-dimensional classification
            10. **K-Nearest Neighbors**: Instance-based learning algorithm
            11. **Decision Tree**: Simple interpretable tree-based model
            12. **Gaussian Naive Bayes**: Probabilistic classifier based on Bayes theorem
            13. **Voting Classifier**: Ensemble method combining multiple models
            14. **Stacking Classifier**: Advanced ensemble with meta-learner
            
            The best performing model is automatically selected based on ROC-AUC score.
            
            ### üìä Key Features
            
            **Comprehensive Data Analysis**
            - 39 unique features analyzed
            - Real-time risk scoring
            - Personalized recommendations
            
            **Advanced Metrics**
            - Financial Health Score
            - Investment Maturity Score
            - Savings Stability Index
            - Debt-to-Income Ratio
            
            **Actionable Insights**
            - Risk categorization (Excellent to Very Poor)
            - Immediate action items
            - Long-term financial planning advice
            
            **Model Performance Analysis**
            - Comprehensive model comparison
            - Performance metrics visualization
            - Feature importance analysis
            - Training time analysis
            
            ### üîí Data Privacy & Security
            
            - All data processing is done in-memory
            - No personal information is stored permanently
            - Compliant with data protection regulations
            - Encrypted data transmission
            
            ### üìà Model Performance
            
            Our ensemble model achieves:
            - **ROC-AUC Score**: > 0.85
            - **Precision**: > 0.80
            - **Recall**: > 0.75
            - **F1-Score**: > 0.77
            
            ### üéì Use Cases
            
            1. **Banks & NBFCs**: Quick credit decisions for loan applications
            2. **Fintech Companies**: Alternative credit scoring for underbanked population
            3. **Individual Users**: Self-assessment of financial health
            4. **Financial Advisors**: Data-driven client recommendations
            
            ### üõ†Ô∏è Technology Stack
            
            - **Frontend**: Streamlit
            - **ML Framework**: Scikit-learn, XGBoost, LightGBM, CatBoost
            - **Data Processing**: Pandas, NumPy
            - **Visualization**: Plotly, Matplotlib, Seaborn
            - **Model Explanation**: SHAP
            - **Imbalance Handling**: SMOTE
            
            ### üìû Support & Feedback
            
            For questions, suggestions, or support:
            - Email: support@creditrisk.ai
            - Documentation: https://docs.creditrisk.ai
            - GitHub: https://github.com/creditrisk
            
            ### ‚ö†Ô∏è Disclaimer
            
            This platform is designed for demonstration and educational purposes. The credit risk scores 
            generated should not be used as the sole basis for financial decisions. Always consult with 
            qualified financial advisors and conduct thorough due diligence before making credit-related decisions.
            
            ### üìú Version Information
            
            - **Version**: 2.0.0
            - **Last Updated**: 2024
            - **Model Training Date**: 2024
            - **Dataset Size**: 5,000 synthetic users
            
            ---
            
            **¬© 2024 Next-Gen Credit Risk Modelling. All rights reserved.
            """)
            
def main():
    """Main function to run the application"""
    if len(sys.argv) > 1 and sys.argv[1] == "streamlit":
        # Run Streamlit app
        app = StreamlitApp()
        app.run()
    else:
        # Run pipeline
        model = CreditRiskModel()
        model.run_pipeline()
        
        print("\n" + "=" * 80)
        print("‚úÖ Setup Complete!")
        print("\nTo launch the Streamlit web app, run:")
        print(f"   streamlit run {sys.argv[0]} streamlit")
        print("=" * 80 + "\n")

if __name__ == "__main__":
    main()